{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pe8rUDv13WU"
      },
      "source": [
        "Copyright 2024 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJURSVoJ1uTL"
      },
      "outputs": [],
      "source": [
        "#@title License\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWcaPMJ_Z19d"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFusDqdUZ19f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gin\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from eq_mag_prediction.forecasting import encoders\n",
        "from eq_mag_prediction.scripts import magnitude_predictor_trainer\n",
        "from eq_mag_prediction.forecasting import training_examples\n",
        "from eq_mag_prediction.forecasting import one_region_model\n",
        "from eq_mag_prediction.utilities import catalog_analysis\n",
        "from eq_mag_prediction.forecasting.training_examples import CatalogDomain\n",
        "from eq_mag_prediction.utilities import data_utils, catalog_filters\n",
        "from eq_mag_prediction.forecasting import metrics\n",
        "from eq_mag_prediction.forecasting.training_examples import CatalogDomain\n",
        "from eq_mag_prediction.forecasting.data_sources import target_catalog\n",
        "from eq_mag_prediction.forecasting.encoders import SeismicityRateEncoder, RecentEarthquakesEncoder, BiggestEarthquakesEncoder, CatalogColumnsEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEYy3xeMZ19h"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UU-xarSZ19h"
      },
      "source": [
        "# Loading and experiment setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NHsGVJcZ19h"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-z90In6Z19i"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'Hauksson'\n",
        "# MODEL_NAME = 'JMA'\n",
        "# MODEL_NAME = 'GeoNet_NZ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "677o8ZvLZ19j"
      },
      "outputs": [],
      "source": [
        "experiment_dir = os.path.join(os.getcwd(), '..', 'results/trained_models/', MODEL_NAME)\n",
        "custom_objects={\n",
        "    '_repeat': encoders._repeat,\n",
        "    }\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    os.path.join(experiment_dir, 'model'),\n",
        "    custom_objects={'_repeat': encoders._repeat},\n",
        "    compile=False,\n",
        "    # safe_mode=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZg1o3NmZ19k"
      },
      "source": [
        "## Set gin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR7aJYjxZ19k"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(experiment_dir, 'config.gin')) as f:\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config(f.read(), skip_unknown=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLsMLPuZZ19l"
      },
      "outputs": [],
      "source": [
        "print(gin.config_str())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkU1wkCyZ19l"
      },
      "outputs": [],
      "source": [
        "gin.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmLHPqbZZ19m"
      },
      "outputs": [],
      "source": [
        "domain = training_examples.CatalogDomain()\n",
        "labels = training_examples.magnitude_prediction_labels(domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRav_icTuPOX"
      },
      "source": [
        "# Set relevant probability density and other definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHXpnl4UAU50"
      },
      "outputs": [],
      "source": [
        "gin_config = gin.config_str()\n",
        "\n",
        "match = re.search(r'train_and_evaluate_magnitude_prediction_model\\.pdf_support_stretch = (\\d+)', gin_config)\n",
        "if match:\n",
        "  stretch = match.group(1)\n",
        "else:\n",
        "  stretch = 7\n",
        "\n",
        "stretch = float(stretch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGVqNmEKZ19m"
      },
      "outputs": [],
      "source": [
        "probability_density_function = metrics.kumaraswamy_mixture_instance\n",
        "LOSS = metrics.MinusLoglikelihoodConstShiftStretchLoss(probability_density_function, domain.magnitude_threshold, stretch)\n",
        "\n",
        "random_var_shift = 0 if not hasattr(LOSS, 'shift') else LOSS.shift\n",
        "random_var_stretch = 7 if not hasattr(LOSS, 'stretch') else LOSS.stretch\n",
        "\n",
        "costum_shift_stretch = lambda x, random_var_shift=random_var_shift, random_var_stretch=random_var_stretch: np.minimum((x - random_var_shift) / random_var_stretch, 1)\n",
        "shift_strech_input = costum_shift_stretch\n",
        "\n",
        "\n",
        "BETA_OF_TRAIN_SET = catalog_analysis.estimate_beta(labels.train_labels, None, 'BPOS')\n",
        "print(BETA_OF_TRAIN_SET)\n",
        "MAG_THRESH = domain.magnitude_threshold\n",
        "print(MAG_THRESH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNSsEcIdZdoy"
      },
      "outputs": [],
      "source": [
        "SET_NAME = 'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yAwSgPL500z"
      },
      "outputs": [],
      "source": [
        "# BETA_OF_TRAIN_SET = catalog_analysis.estimate_beta(labels.train_labels, domain.magnitude_threshold, 'MLE')\n",
        "BETA_OF_TRAIN_SET = catalog_analysis.estimate_beta(labels.train_labels, None, 'BPOS')\n",
        "MAG_THRESH = domain.magnitude_threshold\n",
        "DAY_TO_SECONDS = 60*60*24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og97zyM5tvbC"
      },
      "outputs": [],
      "source": [
        "test_timestamps = np.array(list(domain.test_examples.keys()))\n",
        "validation_timestamps = np.array(list(domain.validation_examples.keys()))\n",
        "train_timestamps = np.array(list(domain.train_examples.keys()))\n",
        "all_timestamps = np.concatenate([train_timestamps, validation_timestamps, test_timestamps])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2uchQ0wawbB"
      },
      "outputs": [],
      "source": [
        "\n",
        "printmd(\n",
        "    f'\u003ch3\u003etrain time:\u003c/h3\u003e {datetime.datetime.fromtimestamp(train_timestamps.min()).__str__()}  -  {datetime.datetime.fromtimestamp(train_timestamps.max()).__str__()}'\n",
        "    )\n",
        "printmd(\n",
        "    f'\u003ch3\u003evalidation time:\u003c/h3\u003e {datetime.datetime.fromtimestamp(validation_timestamps.min()).__str__()}  -  {datetime.datetime.fromtimestamp(validation_timestamps.max()).__str__()}'\n",
        "    )\n",
        "printmd(\n",
        "    f'\u003ch3\u003etest time:\u003c/h3\u003e {datetime.datetime.fromtimestamp(test_timestamps.min()).__str__()}  -  {datetime.datetime.fromtimestamp(test_timestamps.max()).__str__()}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX2Zz8MaZ9tA"
      },
      "outputs": [],
      "source": [
        "set_timestamps = locals()[f'{SET_NAME}_timestamps']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-oc0GTyq8i0"
      },
      "source": [
        "# Get major earthquakes in catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aCdrNd1rHI0"
      },
      "outputs": [],
      "source": [
        "if any(x in MODEL_NAME.lower() for x in ['jma', 'japan']):\n",
        "  major_eq_df = data_utils.japan_major_earthquakes_dataframe()\n",
        "  loaded_catalog = data_utils.jma_dataframe()\n",
        "elif any(x in MODEL_NAME.lower() for x in ['scsn', 'hauksson', 'cali']):\n",
        "  major_eq_df = data_utils.california_major_earthquakes_dataframe()\n",
        "  loaded_catalog = data_utils.hauksson_dataframe()\n",
        "elif any(x in MODEL_NAME.lower() for x in ['nz', 'geonet']):\n",
        "  major_eq_df = data_utils.nz_major_earthquakes_dataframe()\n",
        "  loaded_catalog = data_utils.nz_geonet_dataframe()\n",
        "\n",
        "rows_to_keep = np.isin(loaded_catalog.time.values, set_timestamps)\n",
        "set_catalog = loaded_catalog.copy().iloc[rows_to_keep, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRZ3zjWSrelo"
      },
      "outputs": [],
      "source": [
        "time_logical = (major_eq_df.time \u003e= set_timestamps.min() - DAY_TO_SECONDS) \u0026 (major_eq_df.time \u003c= set_timestamps.max() + DAY_TO_SECONDS)\n",
        "relevant_major_eq = major_eq_df[time_logical]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG0Bwq_L4I2h"
      },
      "outputs": [],
      "source": [
        "relevant_major_eq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oQs5NC7sEHh"
      },
      "outputs": [],
      "source": [
        "time_margin = 2*DAY_TO_SECONDS\n",
        "space_margin = 4\n",
        "mag_margin = 0.8\n",
        "\n",
        "\n",
        "all_event_details = []\n",
        "for event in relevant_major_eq.itertuples():\n",
        "  time_logical = (event.time-time_margin \u003c= set_catalog.time) \u0026 (event.time+time_margin \u003e= set_catalog.time)\n",
        "  if np.isfinite(event.longitude):\n",
        "    lon_logical = (event.longitude-space_margin \u003c= set_catalog.longitude) \u0026 (event.longitude+space_margin \u003e= set_catalog.longitude)\n",
        "  else:\n",
        "    lon_logical = np.full_like(time_logical, True)\n",
        "  if np.isfinite(event.latitude):\n",
        "    lat_logical = (event.latitude-space_margin \u003c= set_catalog.latitude) \u0026 (event.latitude+space_margin \u003e= set_catalog.latitude)\n",
        "  else:\n",
        "    lat_logical = np.full_like(time_logical, True)\n",
        "  mag_logical = (event.magnitude-mag_margin \u003c= set_catalog.magnitude) \u0026 (event.magnitude+mag_margin \u003e= set_catalog.magnitude)\n",
        "  total_logical = time_logical \u0026 lon_logical \u0026 lat_logical \u0026 mag_logical\n",
        "\n",
        "  if not any(total_logical):\n",
        "    print(f'NOTICE!!! no event found for {event.name} magnitude {event.magnitude}')\n",
        "    continue\n",
        "\n",
        "  event_index = set_catalog[total_logical].magnitude.idxmax()\n",
        "  event_details = set_catalog[total_logical].loc[event_index]\n",
        "  event_details['name'] = event.name\n",
        "  event_details['index_in_set'] = np.where(set_catalog.index==event_index)[0][0]\n",
        "  event_details['wiki_magnitude'] = event.magnitude\n",
        "  event_details['wiki_longitude'] = event.longitude\n",
        "  event_details['wiki_latitude'] = event.latitude\n",
        "  event_details['catalog_date'] = datetime.datetime.fromtimestamp(event_details.time).strftime('%Y-%m-%d %H:%M')\n",
        "  event_details['wiki_date'] = datetime.datetime.fromtimestamp(event.time).strftime('%Y-%m-%d %H:%M')\n",
        "  all_event_details.append(event_details)\n",
        "all_event_details = pd.DataFrame(all_event_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I0eRg-zTK5E"
      },
      "source": [
        "# Plot specific distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Ve5NgfwSWX"
      },
      "outputs": [],
      "source": [
        "all_event_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb6kyykLjYO-"
      },
      "outputs": [],
      "source": [
        "\n",
        "scaler_saving_dir = os.path.join(os.getcwd(), '..', f'results/trained_models/{MODEL_NAME}/scalers')\n",
        "all_encoders = one_region_model.build_encoders(domain)\n",
        "\n",
        "one_region_model.compute_and_cache_features_scaler_encoder(\n",
        "    domain,\n",
        "    all_encoders,\n",
        "    force_recalculate = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_T9jUOYZ19p"
      },
      "outputs": [],
      "source": [
        "features_and_models = one_region_model.load_features_and_construct_models(\n",
        "    domain, all_encoders, scaler_saving_dir\n",
        ")\n",
        "train_features = one_region_model.features_in_order(features_and_models, 0)\n",
        "valid_features = one_region_model.features_in_order(features_and_models, 1)\n",
        "test_features = one_region_model.features_in_order(features_and_models, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DP_cx74Z19p"
      },
      "outputs": [],
      "source": [
        "test_forecasts = loaded_model.predict(test_features)\n",
        "plot_above_thresh = MAG_THRESH\n",
        "m_vec = np.linspace(MAG_THRESH, 0.9*(MAG_THRESH+LOSS.stretch), 500)\n",
        "prob_density_inst = probability_density_function(test_forecasts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM5VBBYu28ch"
      },
      "outputs": [],
      "source": [
        "for event in all_event_details.itertuples():\n",
        "  prob_func = lambda m0: prob_density_inst[event.index_in_set].prob((m0 - random_var_shift)/random_var_stretch)/random_var_stretch\n",
        "  prob_vec = prob_func(m_vec)\n",
        "\n",
        "\n",
        "  f_major, ax_major = plt.subplots(1,1, figsize=(5,5))\n",
        "  p = ax_major.plot(m_vec, prob_vec, alpha=0.4, color='r', linewidth=4);\n",
        "  train_gr_curve = metrics.gr_likelihood(m_vec, BETA_OF_TRAIN_SET, MAG_THRESH)\n",
        "  gr_handle = ax_major.plot(m_vec, train_gr_curve, 'k--', label='train_gr_likelihood', linewidth=3)\n",
        "  eq_true_mag = metrics.gr_likelihood(np.array([event.magnitude]), BETA_OF_TRAIN_SET, MAG_THRESH)\n",
        "  ax_major.scatter([event.magnitude], eq_true_mag, s=100, marker='o', edgecolors='k', c='none', linewidths=2)\n",
        "  ax_major.scatter([event.magnitude], [prob_func(event.magnitude).numpy()], s=500, marker='*')\n",
        "  title = f'{event.name}, {event.magnitude:.2f}\\n{datetime.datetime.fromtimestamp(event.time).strftime(\"%d-%b-%y\")}'\n",
        "  ax_major.set_title(title, fontsize=15)\n",
        "  ax_major.set_yscale('log')\n",
        "  #-- set xticks\n",
        "  ax_major.set_xticks([event.magnitude], [f'{event.magnitude:.2f}'], color='red', size=18)\n",
        "  #-- set yticks\n",
        "  ax_major.set_yticks([])\n",
        "  ax_major.tick_params(\n",
        "    axis='y',          # changes apply to the x-axis\n",
        "    which='both',      # both major and minor ticks are affected\n",
        "    left=False,      # ticks along the bottom edge are off\n",
        "    right=False,         # ticks along the top edge are off\n",
        "  )\n",
        "  f_major\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//intelligence/earthquakes/colab:notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1q0aR0y--xDlPjSyebDHneV_NwlymVszh",
          "timestamp": 1720945942915
        },
        {
          "file_id": "142GRBMltYO8FTYzxJba0Nsuc2K6I3rU8",
          "timestamp": 1707679483835
        },
        {
          "file_id": "1x-ci7axx8_HPy-VThOywdPRnL9RCIZ73",
          "timestamp": 1707386996321
        },
        {
          "file_id": "1Ol2MByyPj9rv_dEMpqQWjXfVbrKY2R77",
          "timestamp": 1699277925468
        },
        {
          "file_id": "1X0uCXEsHaG6qH9nZBPo4yVFfjJ9bJySH",
          "timestamp": 1696314810048
        },
        {
          "file_id": "1Pel6FQL10BrAMuX44uk7E8YBtsJgErqA",
          "timestamp": 1691663081634
        },
        {
          "file_id": "1Rdt5eMiL31MblVNFT_GCF25qNGsooh0s",
          "timestamp": 1678092594120
        },
        {
          "file_id": "1cKMZ5nhX_tyW19rw3n1vonhRkLyukGg9",
          "timestamp": 1673957299492
        },
        {
          "file_id": "1Cda53khxHslAUQno6Lw02Ye4q7AWezjm",
          "timestamp": 1673869854946
        },
        {
          "file_id": "1GyuN0CLKvEEb6bufw5LTgqlnSop37Q39",
          "timestamp": 1673786767072
        },
        {
          "file_id": "1f3zM6j4DhjkA0Axhq_zGWSqL7tVK-Ijx",
          "timestamp": 1673444962135
        },
        {
          "file_id": "1FV55Uz5BggBpjsaGszKuXv3sUkWwiujo",
          "timestamp": 1672903019087
        },
        {
          "file_id": "1T7WlP4i5_9xe3H4klPFuIO0a55IN_BMY",
          "timestamp": 1669812340393
        },
        {
          "file_id": "1w-4UtNzqu0Jq0pnZt4gDyQNOX1aMRh_K",
          "timestamp": 1662983276624
        },
        {
          "file_id": "1aqEOiuBdr-UKuymkaqC2XL6psjnInZeJ",
          "timestamp": 1660034359535
        },
        {
          "file_id": "1qT62S_OaBOUQHlKcFnJQGrL9LK_aEPmz",
          "timestamp": 1658839391585
        },
        {
          "file_id": "1S4YbtcFlTECFwhSYu3REOT2D6gzkxJ_O",
          "timestamp": 1655717124037
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
